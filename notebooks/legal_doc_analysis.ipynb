{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# **1. Data Loading and Overview**\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import nltk\n","nltk.download('punkt_tab')\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Load dataset\n","data = pd.read_csv('NLP_Dataset_Extended.csv')\n","data.rename(columns={'Input': 'Health_Record', 'Prediction': 'Risk_Level'}, inplace=True)\n","\n","print(\"\\n✅ Dataset Overview:\")\n","print(data.info())\n","\n","# Class distribution\n","plt.figure(figsize=(8, 5))\n","sns.countplot(x='Risk_Level', data=data, palette='coolwarm')\n","plt.title('Class Distribution')\n","plt.show()\n","\n","# Train-validation-test split\n","train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Risk_Level'], random_state=42)\n","train_data, val_data = train_test_split(train_data, test_size=0.1, stratify=train_data['Risk_Level'], random_state=42)\n"],"metadata":{"id":"mUMnfb8FFUgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **2. Preprocessing**\n","\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    tokens = word_tokenize(text)\n","    important_terms = {\"diabetes\", \"cholesterol\", \"blood\", \"pressure\", \"heart\", \"disease\", \"smoker\"}\n","    tokens = [word for word in tokens if word not in stop_words or word in important_terms]\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return ' '.join(tokens)\n","\n","# Apply to all splits\n","train_data['processed_text'] = train_data['Health_Record'].apply(preprocess_text)\n","val_data['processed_text'] = val_data['Health_Record'].apply(preprocess_text)\n","test_data['processed_text'] = test_data['Health_Record'].apply(preprocess_text)\n"],"metadata":{"id":"sq4ZQcEoFVt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","def preprocess_text(text):\n","    # Sentence Tokenization\n","    sentences = sent_tokenize(text)\n","\n","    all_tokens = []\n","    for sentence in sentences:\n","        # Word Tokenization and other preprocessing steps\n","        text = sentence.lower()\n","        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","        tokens = word_tokenize(text)\n","        important_terms = {\"diabetes\", \"cholesterol\", \"blood\", \"pressure\", \"heart\", \"disease\", \"smoker\"}\n","        tokens = [word for word in tokens if word not in stop_words or word in important_terms]\n","        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","        all_tokens.extend(tokens) # Add tokens from this sentence\n","\n","    return ' '.join(all_tokens)"],"metadata":{"id":"YnhyfvntOVyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertModel\n","# ... (Import statements and other code)\n","\n","bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Fine-tuning BERT\n","bert_model.trainable = True  # Make BERT layers trainable\n","# ... (Rest of your model training code)"],"metadata":{"id":"9EcC7JoHOiol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_bert_embeddings(texts, tokenizer, model, max_len=128):\n","    # ... (Tokenization and BERT model call)\n","\n","    # Average token embeddings for sentence-level representation\n","    embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n","    return embeddings.numpy()"],"metadata":{"id":"C2uSh3-6OlTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# **3. BERT Embeddings**\n","\n","from transformers import BertTokenizer, TFBertModel\n","import tensorflow as tf\n","import numpy as np\n","\n","bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def get_bert_embeddings(texts, tokenizer, model, max_len=128):\n","    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\", max_length=max_len)\n","    outputs = model(tokens)\n","    embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n","    return embeddings.numpy()\n","\n","X_train_embeddings = get_bert_embeddings(train_data['processed_text'].tolist(), tokenizer, bert_model)\n","X_val_embeddings = get_bert_embeddings(val_data['processed_text'].tolist(), tokenizer, bert_model)\n","X_test_embeddings = get_bert_embeddings(test_data['processed_text'].tolist(), tokenizer, bert_model)\n"],"metadata":{"id":"G5Rd6levFZTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Bidirectional, Flatten, Input\n","from keras.callbacks import EarlyStopping\n","\n","# Encode target labels\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(train_data['Risk_Level'])\n","y_val = label_encoder.transform(val_data['Risk_Level'])\n","y_test = label_encoder.transform(test_data['Risk_Level'])\n","\n","# Reshape for CNN/LSTM input\n","X_train = X_train_embeddings.reshape((X_train_embeddings.shape[0], 1, X_train_embeddings.shape[1]))\n","X_val = X_val_embeddings.reshape((X_val_embeddings.shape[0], 1, X_val_embeddings.shape[1]))\n","X_test = X_test_embeddings.reshape((X_test_embeddings.shape[0], 1, X_test_embeddings.shape[1]))\n","\n","input_shape = (1, 768)\n","\n","# Class imbalance weights\n","class_weights = {0: 1.0, 1: 308 / 92}\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","def cnn_bilstm_model(input_shape):\n","    model = Sequential()\n","    model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling1D(pool_size=1))\n","    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Train the model\n","model = cnn_bilstm_model(input_shape)\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64,\n","          class_weight=class_weights, callbacks=[early_stopping])\n"],"metadata":{"id":"gGOkguOzFbw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **5. Evaluation**\n","\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","preds = model.predict(X_test)\n","acc = accuracy_score(y_test, preds.round())\n","print(f\"\\n✅ BERT + CNN-BiLSTM Accuracy: {acc:.4f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, preds.round()))\n"],"metadata":{"id":"HECfouSxFf58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **6. Save & Predict**\n","\n","model.save('bert_cnn_bilstm_model.h5')\n","\n","# Predict new input\n","def predict_risk_level(text, tokenizer, bert_model, model):\n","    processed = preprocess_text(text)\n","    emb = get_bert_embeddings([processed], tokenizer, bert_model)\n","    emb = emb.reshape(1, 1, 768)\n","    pred = model.predict(emb)\n","    label_map = {0: \"low-risk\", 1: \"high-risk\"}\n","    return label_map[int(pred.round()[0][0])], pred[0][0]\n","\n","example = \"Patient is a 60-year-old smoker with high cholesterol and blood pressure.\"\n","label, probability = predict_risk_level(example, tokenizer, bert_model, model)\n","print(f\"Predicted Class: {label} (Probability: {probability:.2f})\")\n"],"metadata":{"id":"h2ITMQGyFhsE"},"execution_count":null,"outputs":[]}]}