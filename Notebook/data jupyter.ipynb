
# --- Cell 1 ---
!pip install transformers datasets evaluate gradio -q

# --- Cell 2 ---
from google.colab import files
uploaded = files.upload()  # select your dataset file(s), e.g. legal_data.xlsx or legal_data.csv

# --- Cell 3 ---
import pandas as pd

# Load your dataset
try:
    df = pd.read_csv("legal_docs_labelled355.csv", encoding='latin-1')
except UnicodeDecodeError:
    try:
        df = pd.read_csv("legal_docs_labelled355.csv", encoding='cp1252')
    except UnicodeDecodeError:
        print("Could not decode the file with 'latin-1' or 'cp1252' encoding. Please check the file encoding.")
        df = None # Ensure df is None if decoding fails


# Show first rows
if df is not None:
    print(df.head())

    # Show column names
    print("\nColumns:", df.columns.tolist())

    # Show unique labels (if column exists)
    for col in df.columns:
        print(f"\nColumn: {col}")
        # Check if the column is not empty before printing unique values
        if not df[col].empty:
            print(df[col].unique()[:10])  # print first 10 unique values
        else:
            print("Column is empty.")

# --- Cell 4 ---
import pandas as pd

# Load your dataset with specified encoding
try:
    df = pd.read_csv("legal_docs_labelled355.csv", encoding='latin-1')
except UnicodeDecodeError:
    try:
        df = pd.read_csv("legal_docs_labelled355.csv", encoding='cp1252')
    except UnicodeDecodeError:
        print("Could not decode the file with 'latin-1' or 'cp1252' encoding. Please check the file encoding.")
        df = None # Ensure df is None if decoding fails


if df is not None:
    # Drop rows with missing values in important columns
    df = df.dropna(subset=["clause_text", "clause_type"])

    print(df["clause_type"].value_counts())

# --- Cell 5 ---
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["label"] = le.fit_transform(df["clause_type"])  # convert text labels to numbers

num_labels = df["label"].nunique()
print("Number of classes:", num_labels)

# --- Cell 6 ---
from datasets import Dataset

dataset = Dataset.from_pandas(df[["clause_text", "label"]])
dataset

# --- Cell 7 ---
# Train/test split
dataset = dataset.train_test_split(test_size=0.2)
dataset

# --- Cell 8 ---
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["clause_text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets

# --- Cell 9 ---
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# --- Cell 10 ---
import evaluate

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy.compute(predictions=predictions, references=labels)
    f1_score = f1.compute(predictions=predictions, references=labels, average="weighted")
    return {"accuracy": acc["accuracy"], "f1": f1_score["f1"]}

# --- Cell 11 ---
import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

training_args = TrainingArguments(
    output_dir="test_trainer",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# --- Cell 12 ---
trainer.train()

# --- Cell 13 ---
results = trainer.evaluate()
print(results)

# --- Cell 14 ---
predictions = trainer.predict(tokenized_datasets["test"])
preds = predictions.predictions.argmax(axis=-1)

print("Sample predictions:", preds[:10])
print("Actual labels:", predictions.label_ids[:10])

# --- Cell 15 ---
text = "This agreement shall be governed by and construed in accordance with the laws of India."
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class = logits.argmax().item()
print("Predicted class:", le.inverse_transform([predicted_class])[0])

# --- Cell 16 ---
import gradio as gr

def classify_clause(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_class = logits.argmax().item()
    return le.inverse_transform([predicted_class])[0]

demo = gr.Interface(
    fn=classify_clause,
    inputs="text",
    outputs="text",
    title="Legal Clause Classifier",
    description="Enter a legal clause and the model will predict its type."
)

# --- Cell 17 ---
demo.launch(share=True)

# --- Cell 18 ---
df.head()

# --- Cell 19 ---
df["clause_type"].value_counts()

# --- Cell 20 ---
df.shape

# --- Cell 21 ---
df.isnull().sum()

# --- Cell 22 ---
df["clause_type"].nunique()

# --- Cell 23 ---
import matplotlib.pyplot as plt

df["clause_type"].value_counts().plot(kind="bar", figsize=(12,6))
plt.title("Distribution of Clause Types")
plt.xlabel("Clause Type")
plt.ylabel("Count")
plt.show()

# --- Cell 24 ---
import seaborn as sns

sns.countplot(x=df["clause_type"])
plt.xticks(rotation=90)
plt.show()

# --- Cell 25 ---
from sklearn.model_selection import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df["clause_text"].tolist(),
    df["label"].tolist(),
    test_size=0.2,
    random_state=42
)

print("Train size:", len(train_texts))
print("Test size:", len(test_texts))

# --- Cell 26 ---
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)

# --- Cell 27 ---
import torch

class LegalDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = LegalDataset(train_encodings, train_labels)
test_dataset = LegalDataset(test_encodings, test_labels)

len(train_dataset), len(test_dataset)

# --- Cell 28 ---
from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

# --- Cell 29 ---
batch = next(iter(train_loader))
{k: v.shape for k, v in batch.items()}

# --- Cell 30 ---
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

# --- Cell 31 ---
from torch.nn import CrossEntropyLoss

loss_fn = CrossEntropyLoss()

# --- Cell 32 ---
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# --- Cell 33 ---
from tqdm import tqdm

model.train()
for epoch in range(1):
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        loop.set_description(f"Epoch {epoch}")
        loop.set_postfix(loss=loss.item())

# --- Cell 34 ---
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

accuracy = correct / total
print("Test Accuracy:", accuracy)

# --- Cell 35 ---
sample_text = "The lessee shall pay rent on or before the 5th of each month."
inputs = tokenizer(sample_text, return_tensors="pt").to(device)

with torch.no_grad():
    logits = model(**inputs).logits
predicted_class = logits.argmax().item()
print("Predicted:", le.inverse_transform([predicted_class])[0])

# --- Cell 36 ---
import numpy as np
from sklearn.metrics import classification_report

y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

print(classification_report(y_true, y_pred, target_names=le.classes_))

# --- Cell 37 ---
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(12,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# --- Cell 38 ---
model.save_pretrained("legal_model")
tokenizer.save_pretrained("legal_model")

# --- Cell 39 ---
from transformers import pipeline

classifier = pipeline("text-classification", model="legal_model", tokenizer="legal_model")

classifier("The parties agree to resolve disputes by arbitration.")

# --- Cell 40 ---
import random

samples = random.sample(df["clause_text"].tolist(), 5)
for s in samples:
    print(s)
    print("Prediction:", classifier(s))
    print("----")

# --- Cell 41 ---
import torch

model.eval()
for s in samples:
    inputs = tokenizer(s, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
    pred = logits.argmax().item()
    print(f"Text: {s}\nPredicted: {le.inverse_transform([pred])[0]}\n")

# --- Cell 42 ---
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

X = df["clause_text"]
y = df["label"]

# Use embeddings from tokenizer for classical ML baseline
X_enc = tokenizer(X.tolist(), truncation=True, padding=True, max_length=128, return_tensors="np")["input_ids"]

clf = LogisticRegression(max_iter=1000)
scores = cross_val_score(clf, X_enc, y, cv=3, scoring="accuracy")
print("Baseline Logistic Regression Accuracy:", scores.mean())

# --- Cell 43 ---
print("Notebook execution complete!")
